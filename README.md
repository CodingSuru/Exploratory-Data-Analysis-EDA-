# Exploratory Data Analysis (EDA) on AI Job Dataset ğŸ“Š

## Purpose ğŸ¯
This Jupyter notebook conducts an **Exploratory Data Analysis (EDA)** on the "AI Job Dataset" to uncover insights, patterns, and relationships within the data. The primary goals are to:
- Understand the structure and characteristics of the dataset ğŸ“‹.
- Identify key trends, distributions, and correlations in AI-related job postings ğŸ”.
- Visualize relationships between variables, such as experience level and years of experience ğŸ“ˆ.
- Prepare the data for potential future analysis or modeling by cleaning and transforming it ğŸ§¹.
- Answer specific questions about the dataset, such as the average years of experience by experience level â“.

The notebook serves as an educational tool for data analysts and scientists to practice EDA techniques and gain insights into the AI job market ğŸš€.

## Dataset ğŸ“‚
- **Source**: The dataset is sourced from `'C:\Users\suryansh\Downloads\archive\ai_job_dataset.csv'`.
- **Description**: Contains 15,000 job postings related to AI roles, with details on job titles, salaries, experience levels, employment types, company locations, and more.
- **Columns**:
  - `job_id`: Unique identifier for each job posting ğŸ†”.
  - `job_title`: Title of the job (e.g., AI Research Scientist, Data Analyst) ğŸ’¼.
  - `salary_usd`: Salary in USD (or other currency as specified) ğŸ’°.
  - `salary_currency`: Currency of the salary ğŸ’¸.
  - `experience_level`: Level of experience (e.g., EN for Entry, SE for Senior) ğŸ“š.
  - `employment_type`: Type of employment (e.g., FT for Full-Time, PT for Part-Time) â°.
  - `company_location`: Location of the company ğŸŒ.
  - `company_size`: Size of the company (S, M, L) ğŸ¢.
  - `employee_residence`: Residence of the employee ğŸ .
  - `remote_ratio`: Percentage of remote work allowed ğŸŒ.
  - `required_skills`: List of required skills ğŸ› ï¸.
  - `education_required`: Required education level ğŸ“.
  - `years_experience`: Years of experience required â³.
  - `industry`: Industry of the job ğŸ­.
  - `posting_date`: Date the job was posted ğŸ“….
  - `application_deadline`: Application deadline â³.
  - `job_description_length`: Length of the job description ğŸ“œ.
  - `benefits_score`: Score representing job benefits ğŸŒŸ.
  - `company_name`: Name of the company ğŸ¬.

## Structure ğŸ—‚ï¸
The notebook is organized into the following steps:
1. **Imports and Reading Data** ğŸ“¥:
   - Imports necessary Python libraries (`pandas`, `numpy`, `matplotlib`, `seaborn`).
   - Loads the dataset into a pandas DataFrame.
2. **Data Understanding** ğŸ”:
   - Examines the DataFrame's shape, head, tail, data types, and descriptive statistics.
3. **Data Preparation** ğŸ› ï¸:
   - Renames columns for consistency (e.g., `Years_Experience` to `years_experience`).
   - Drops irrelevant columns or duplicates.
   - Handles missing values and creates new features if needed.
4. **Feature Understanding** ğŸ“‰:
   - Conducts univariate analysis using visualizations like value counts, KDE plots, and boxplots to explore individual feature distributions.
5. **Feature Relationships** ğŸ”—:
   - Performs bivariate analysis with scatter plots, pairplots, and correlation heatmaps to identify relationships between variables.
6. **Asking a Question** â“:
   - Analyzes the average years of experience by experience level and visualizes it using a horizontal bar plot.

## Requirements ğŸ› ï¸
To run the notebook, ensure the following are installed:
- **Python**: Version 3.x ğŸ
- **Libraries**:
  - `pandas`: For data manipulation and analysis ğŸ“‹.
  - `numpy`: For numerical operations ğŸ”¢.
  - `matplotlib`: For creating visualizations ğŸ“Š.
  - `seaborn`: For enhanced statistical visualizations ğŸ¨.

Install dependencies using:
```bash
pip install pandas numpy matplotlib seaborn
